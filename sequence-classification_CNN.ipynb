{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "path = \"./home/users/khwong352/FYP/ee5434/\"\n",
    "def read_data(partition , data_path=path):\n",
    "  data = []\n",
    "  for fn in os.listdir(os.path.join(data_path, partition)):\n",
    "    with open(os.path.join(data_path, partition, fn)) as f:\n",
    "      data.append(pd.read_csv(f, index_col=None))\n",
    "  return pd.concat(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all data_partitions\n",
    "# path = \"../input/pfam-seed-random-split/random_split/random_split\"\n",
    "df_test = pd.read_csv('/home/users/khwong352/FYP/ee5434/test.csv')\n",
    "df_val = pd.read_csv('/home/users/khwong352/FYP/ee5434/val.csv')\n",
    "df_train = pd.read_csv('/home/users/khwong352/FYP/ee5434/train.csv')\n",
    "df_sample = pd.read_csv('/home/users/khwong352/FYP/ee5434/sampleSubmission.csv')\n",
    "\n",
    "#df_train.head(5)\n",
    "#print(df_train['family_accession'].unique())\n",
    "#df_train['family_accession'].nunique()\n",
    "#print(df_train)\n",
    "#print(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TAATAATTCGCCAATTAAATCATATTTTTAATATGATTTAATTGGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ATTATTTTCCTTGTAGAATTTGGTCTGGACTTTTGCCCTGTGCAAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GGCGCTGTGCTGCTTCTTGGCCGGCGCGGCCGACGCCTACCGGTAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TACGTTTATTTTGGATCCATTCGATCCCCTGAGTCACCATTGGCAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GAAAAGATGCGCGCGACCATTTCGTCGGGCAGCGTCGCGAACAGGT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              reads\n",
       "0   0  TAATAATTCGCCAATTAAATCATATTTTTAATATGATTTAATTGGC...\n",
       "1   1  ATTATTTTCCTTGTAGAATTTGGTCTGGACTTTTGCCCTGTGCAAT...\n",
       "2   2  GGCGCTGTGCTGCTTCTTGGCCGGCGCGGCCGACGCCTACCGGTAC...\n",
       "3   3  TACGTTTATTTTGGATCCATTCGATCCCCTGAGTCACCATTGGCAG...\n",
       "4   4  GAAAAGATGCGCGCGACCATTTCGTCGGGCAGCGTCGCGAACAGGT..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41211"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unique_cls(train, test, val):\n",
    "  \"\"\"\n",
    "  Prints # unique classes in data sets.\n",
    "  \"\"\"\n",
    "  train_unq = np.unique(train['label'].values)\n",
    "  val_unq = np.unique(val['label'].values)\n",
    "  test_unq = np.unique(test['ID'].values)\n",
    "\n",
    "  print('Number of unique classes in Train: ', len(train_unq))\n",
    "  print('Number of unique classes in Val: ', len(val_unq))\n",
    "  print('Number of unique classes in Test: ', len(test_unq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = df_train['label'].value_counts()[:1000].index.tolist()\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size after considering 1000 classes for each data split:\n",
      "Train size : 1321357\n",
      "Val size : 170665\n",
      "Test size : 3\n"
     ]
    }
   ],
   "source": [
    "# Filtering data based on considered 1000 classes.\n",
    "train_sm = df_train.loc[df_train['label'].isin(classes)].reset_index()\n",
    "val_sm = df_val.loc[df_val['label'].isin(classes)].reset_index()\n",
    "test_sm = df_test.loc[df_test['ID'].isin(classes)].reset_index()\n",
    "\n",
    "print('Data size after considering 1000 classes for each data split:')\n",
    "print('Train size :', len(train_sm))\n",
    "print('Val size :', len(val_sm))\n",
    "print('Test size :', len(test_sm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes in Train:  3\n",
      "Number of unique classes in Val:  3\n",
      "Number of unique classes in Test:  3\n"
     ]
    }
   ],
   "source": [
    "calc_unique_cls(train_sm, test_sm, val_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20}\n"
     ]
    }
   ],
   "source": [
    "codes = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "def create_dict(codes):\n",
    "  char_dict = {}\n",
    "  for index, val in enumerate(codes):\n",
    "    char_dict[val] = index+1\n",
    "\n",
    "  return char_dict\n",
    "\n",
    "char_dict = create_dict(codes)\n",
    "\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def integer_encoding(data):\n",
    "  \"\"\"\n",
    "  - Encodes code sequence to integer values.\n",
    "  - 20 common amino acids are taken into consideration\n",
    "    and rest 4 are categorized as 0.\n",
    "  \"\"\"\n",
    "\n",
    "  encode_list = []\n",
    "  for row in data['reads']:\n",
    "    row_encode = []\n",
    "    for code in row:\n",
    "      row_encode.append(char_dict.get(code, 0))\n",
    "    encode_list.append(np.array(row_encode))\n",
    "\n",
    "  return encode_list\n",
    "\n",
    "train_encode = integer_encoding(train_sm) \n",
    "val_encode = integer_encoding(val_sm) \n",
    "test_encode = integer_encoding(test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_encode[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1321357, 100) (170665, 100) (3, 100)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.kears.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence as sequence\n",
    "\n",
    "max_length = 100\n",
    "train_pad = sequence.pad_sequences(train_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "val_pad = sequence.pad_sequences(val_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "test_pad = sequence.pad_sequences(test_encode, maxlen=max_length, padding='post', truncating='post')\n",
    "#print(train_pad)\n",
    "print(train_pad.shape, val_pad.shape, test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1321357, 100, 18), (3, 100, 18), (3, 100, 18))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One hot encoding of sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_ohe = to_categorical(train_pad)\n",
    "val_ohe = to_categorical(val_pad)\n",
    "test_ohe = to_categorical(test_pad)\n",
    "\n",
    "train_ohe.shape, test_ohe.shape, test_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1321357,), (170665,), (3,))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label/integer encoding output variable: (y)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train_le = le.fit_transform(train_sm['label'])\n",
    "y_val_le = le.transform(val_sm['label'])\n",
    "y_test_le = le.transform(test_sm['ID'])\n",
    "\n",
    "y_train_le.shape, y_val_le.shape, y_test_le.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes:  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Total classes: ', len(le.classes_))\n",
    "# le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1321357, 3), (170665, 3), (3, 3))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One hot encoding of outputs\n",
    "y_train = to_categorical(y_train_le)\n",
    "y_val = to_categorical(y_val_le)\n",
    "y_test = to_categorical(y_test_le)\n",
    "\n",
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: plot model's accuracy and loss\n",
    "\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "  acc = history.history['acc']\n",
    "  val_acc = history.history['val_acc']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  x = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, acc, 'b', label='Training acc')\n",
    "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, loss, 'b', label='Training loss')\n",
    "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: Display model score(Loss & Accuracy) across all sets.\n",
    "\n",
    "def display_model_score(model, train, val, test, batch_size):\n",
    "\n",
    "  train_score = model.evaluate(train[0], train[1], batch_size=batch_size, verbose=1)\n",
    "  print('Train loss: ', train_score[0])\n",
    "  print('Train accuracy: ', train_score[1])\n",
    "  print('-'*70)\n",
    "\n",
    "  val_score = model.evaluate(val[0], val[1], batch_size=batch_size, verbose=1)\n",
    "  print('Val loss: ', val_score[0])\n",
    "  print('Val accuracy: ', val_score[1])\n",
    "  print('-'*70)\n",
    "  \n",
    "  test_score = model.evaluate(test[0], test[1], batch_size=batch_size, verbose=1)\n",
    "  print('Test loss: ', test_score[0])\n",
    "  print('Test accuracy: ', test_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(data, filters, d_rate):\n",
    "  \"\"\"\n",
    "  _data: input\n",
    "  _filters: convolution filters\n",
    "  _d_rate: dilation ratees\n",
    "  \"\"\"\n",
    "\n",
    "  shortcut = data\n",
    "\n",
    "  bn1 = BatchNormalization()(data)\n",
    "  act1 = Activation('relu')(bn1)\n",
    "  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n",
    "\n",
    "  #bottleneck convolution\n",
    "  bn2 = BatchNormalization()(conv1)\n",
    "  act2 = Activation('relu')(bn2)\n",
    "  conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n",
    "\n",
    "  #skip connection\n",
    "  x = Add()([conv2, shortcut])\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 100, 18)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, 100, 128)     2432        input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 100, 128)     512         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 100, 128)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, 100, 128)     16512       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 100, 128)     512         conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 100, 128)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, 100, 128)     49280       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 100, 128)     0           conv1d_46[0][0]                  \n",
      "                                                                 conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 100, 128)     512         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 100, 128)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, 100, 128)     16512       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 100, 128)     512         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 100, 128)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, 100, 128)     49280       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 100, 128)     0           conv1d_48[0][0]                  \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 33, 128)      0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 33, 128)      0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 4224)         0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 3)            12675       flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 148,739\n",
      "Trainable params: 147,715\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "x_input = Input(shape=(100, 18))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(128, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 128, 2)\n",
    "res2 = residual_block(res1, 128, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "x_output = Dense(3, activation='softmax', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model2 = Model(inputs=x_input, outputs=x_output)\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5162/5162 [==============================] - 717s 137ms/step - loss: 0.6400 - accuracy: 0.7933 - val_loss: 0.5183 - val_accuracy: 0.8596\n",
      "Epoch 2/10\n",
      "5162/5162 [==============================] - 697s 135ms/step - loss: 0.5076 - accuracy: 0.8056 - val_loss: 0.6340 - val_accuracy: 0.8639\n",
      "Epoch 3/10\n",
      "5162/5162 [==============================] - 684s 133ms/step - loss: 0.4954 - accuracy: 0.8091 - val_loss: 0.5927 - val_accuracy: 0.8617\n",
      "Epoch 4/10\n",
      "5162/5162 [==============================] - 695s 135ms/step - loss: 0.4913 - accuracy: 0.8106 - val_loss: 0.6548 - val_accuracy: 0.8604\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history2 = model2.fit(\n",
    "    train_ohe, y_train,\n",
    "    epochs=10, batch_size=256,\n",
    "    validation_data=(val_ohe, y_val),\n",
    "    callbacks=[es]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model weights.\n",
    "model2.save_weights('/home/users/khwong352/FYP/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-649017dd702f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-4a1fc690cb54>\u001b[0m in \u001b[0;36mplot_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5162/5162 [==============================] - 261s 51ms/step - loss: 0.4938 - accuracy: 0.8092\n",
      "Train loss:  0.49376341700553894\n",
      "Train accuracy:  0.8092150688171387\n",
      "----------------------------------------------------------------------\n",
      "667/667 [==============================] - 35s 52ms/step - loss: 0.6548 - accuracy: 0.8604\n",
      "Val loss:  0.6548165678977966\n",
      "Val accuracy:  0.8603697419166565\n",
      "----------------------------------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8147 - accuracy: 0.6667\n",
      "Test loss:  0.8146513104438782\n",
      "Test accuracy:  0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "\n",
    "display_model_score(\n",
    "    model2,\n",
    "    [train_ohe, y_train],\n",
    "    [val_ohe, y_val],\n",
    "    [test_ohe, y_test],\n",
    "    256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+---------+----------+\n",
      "| Sr.no |       Model        | Train Acc | Val Acc | Test Acc |\n",
      "+-------+--------------------+-----------+---------+----------+\n",
      "|   1.  | Bidirectional LSTM |   0.964   |  0.957  |  0.958   |\n",
      "|   2.  |      ProtCNN       |   0.996   |  0.988  |  0.988   |\n",
      "+-------+--------------------+-----------+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = ['Sr.no', 'Model', 'Train Acc', 'Val Acc','Test Acc']\n",
    "\n",
    "x.add_row(['1.', 'Bidirectional LSTM', '0.964', '0.957', '0.958'])\n",
    "x.add_row(['2.', 'ProtCNN', '0.996', '0.988', '0.988'])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
